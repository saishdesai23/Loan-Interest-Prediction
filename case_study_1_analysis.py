# -*- coding: utf-8 -*-
"""case_study_1_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d-CrZFGKHseE91hICeuAyfZqLmH3tMZ1

# Case study 1

## 1. Importing python libraries
"""

# importing all the required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# %%capture
# !pip install kaleido
# !pip install plotly>=4.0.0
# !wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca
# !chmod +x /usr/local/bin/orca
# !apt-get install xvfb libgtk2.0-0 libgconf-2-4

"""## 2. Loading the data from a csv file into a pandas dataframe"""

# importing the data set
loan_data = pd.read_csv("/content/loans_full_schema.csv")
loan_data.head()

"""## 3. Describing the data set"""

# Stats of the numerical variables in the dataframe
df1_transposed = loan_data.describe().T
df1_transposed = df1_transposed.reset_index()
df1_transposed.rename(columns={"index":"Column_name"}, inplace=True)

# Describing the details of each column in a data frame
l1 = list(loan_data.columns)
l2 = list(loan_data.dtypes)
l3 = list(loan_data.isna().sum())
data_describe = pd.DataFrame(zip(l1,l2,l3),columns =['Column_name', 'Data_Type','Null_count'])
data_describe['isnull'] = np.where(data_describe['Null_count'] !=0,'T','F')
data_describe['Null_percent'] = round((data_describe['Null_count']*100)/len(loan_data),3)
data_describe['Non_num'] = np.where(data_describe['Data_Type'] =='object','T','F')
data_describe = data_describe.merge(df1_transposed, on='Column_name', how='left')
data_describe

# listing columns with missing values
data_missing = data_describe[data_describe['isnull'] == 'T']
data_missing

# listing columns with object datatype
data_object = data_describe[data_describe['Non_num'] == 'T']
data_object

"""We describe the data by listing all the parameters and properties of the columns (predictors) in a separate dataframe.

The properties we describe for each column are


*   Data type
*   Null count
*   Null percent
*   Statistical parameters for numerical variables

We also created a separate dataframw to list of the missing variables and variables with non-numerical datatype

## 4. Data Visualization

### 1. Visualizing the data for different states within the United States. 

The data set has loans made through the Lending Club platform for all the states except 'IOWA'.

Here, we will plot the statewise distribution of 3 factors which can serve as major contributors in predicting the interest rate-

1.   anual_income - anual income of the employee
2.   public_record_bankrupt - Number of bankruptcies listed in the public record for this applicant.
2.   delinq_2y - Delinquencies on lines of credit in the last 2 years.
"""

# Total value of 'annual_income','public_record_bankrupt' and 'delinq_2y' per state
loan_region = loan_data.groupby(['state']).sum().reset_index()
loan_region = loan_region[['state','annual_income','public_record_bankrupt','delinq_2y']]
print(loan_region.head(5))

# Number of applicants per state
applicant_count = loan_data.groupby(['state']).count().reset_index()['interest_rate']
print(applicant_count.head(5))

# Normalizing the total value of all the factors with the loan approval count per state
print("Normalized values")
loan_region['annual_income'] = round(loan_region['annual_income']/applicant_count,2)
loan_region['public_record_bankrupt'] = round(loan_region['public_record_bankrupt']/applicant_count,2)
loan_region['delinq_2y'] = round(loan_region['delinq_2y']/applicant_count,2)
print(loan_region.head(5))

para = ['annual_income','public_record_bankrupt','delinq_2y']
for ele in para:
  print(ele.title()+ " per loan approval across all states in the United States")
  l = list(loan_region[ele])
  fig = px.choropleth(loan_region, color = list(loan_region[ele]),
                           locations=list(loan_region['state']),
                           locationmode="USA-states",
                           color_continuous_scale="Darkmint",
                           range_color=(min(l), max(l)),
                           hover_name="state",
                           scope="usa",
                           labels={'color': ele.lower() + " per loan approval", 'locations':'states'}
                          )
  
  fig.show()

"""To normalize and compare the distribution we have calculated the total value of each factor and divded it with the loan appoval count for each state.

#### Important Observations
The total __"annual income"__ for the state 'CA' is __"USD 1.182145e+08"__ and the approval count is __"1330"__. So Annual Income per applicant will be __"USD 88883.048767"__.
However the __"public_record_bankrupt"__  and __"deliquencies"__ are sufficently high for this state.

The Annual Income per loan approval is high for "WY". This is because either there are very few applicants in this state or most of the loans application might have got rejected.

Thus, based on these factors across the states, we can put them into different slabs of loan schemes, where in riskier states can be alloted higher loan rates.

### 2. Loan Disbursement analysis based on the loan application reason

Each loan disbursement is based on a purpose. This purpose can be any product or service which the loan borrower needs to avial. Below we have a bar plot visualizing the average amount of loan disbursed for each loan purpose.
"""

loan_reason = loan_data.groupby(['loan_purpose']).mean().reset_index()
loan_reason.sort_values(by='loan_amount', inplace=True)


# adjusting the color density of each bar in the plot based on the loan amount value
normalized_df=(loan_reason['loan_amount']-loan_reason['loan_amount'].min())/(loan_reason['loan_amount'].max()-loan_reason['loan_amount'].min())
normalized_df = normalized_df.to_numpy()
rgba_colors = np.zeros((12,4))
rgba_colors[:,0]=0.92968  #value of red intensity divided by 256 
rgba_colors[:,1]=0.12890  #value of green intensity divided by 256
rgba_colors[:,2]=0.10546  #value of blue intensity divided by 256
rgba_colors[:,-1]=normalized_df.reshape(1,12).flatten() # adding values of normalized loan amount (height)

plt.figure(figsize = (20,8))
plt.bar(loan_reason['loan_purpose'], height= loan_reason['loan_amount'], width=0.5, bottom=None, color = rgba_colors, edgecolor='black')


# reference - https://stackoverflow.com/questions/53066633/python-how-to-show-values-on-top-of-bar-plot
xlocs = plt.xticks()
xlocs=[i+1 for i in range(0,12)]
for i, v in enumerate(loan_reason['loan_amount']):
    plt.text(xlocs[i]-1.3, v + 400, str(round(v,2)),fontsize=16)

plt.title("Average Loan Amount for different categories of loan purpose", fontsize=16)
plt.xlabel('Loan Categories',fontsize=18)
plt.ylabel('Average loan disbursement amount',fontsize=18)
plt.xticks(loan_reason['loan_purpose'], rotation =45, fontsize=16)
plt.yticks(fontsize=16)
plt.show()

"""From this plot we can conclude that __"debt_consolidation"__, __"house"__ and __"small_business"__ are three major reason for loan.

People borrowing loan for above 3 reasons should be targeted and given low interest rates to bring greater profits to the lending business.

### 3. Proportions of different grade of loans

The Adjustment for Risk & Volatility is designed to cover expected losses and provide higher risk-adjusted returns for each loan grade increment from A1 to G5.

Considering this statement, we assume that the loan risk will be graded lowest for __"A"__ and highest for __"G"__

reference article to help the labeliing of the pie chart -https://stackoverflow.com/questions/64411633/how-to-rotate-the-percentage-label-in-a-pie-chart-to-match-the-category-label-ro
"""

# https://stackoverflow.com/questions/64411633/how-to-rotate-the-percentage-label-in-a-pie-chart-to-match-the-category-label-ro
loan_grade = loan_data.groupby(['grade']).count().reset_index()
colors = sns.color_palette('colorblind')[0:7]

#creating the pie chart
plt.figure(figsize = (10,10))
patches, labels, pct_texts = plt.pie(loan_grade['loan_amount'], 
                                     colors = colors, 
                                     autopct='%.0f%%',
                                     rotatelabels=True, 
                                     pctdistance=1.2,
                                     wedgeprops={"edgecolor":"k",'linewidth': 2, 'antialiased': True})
# labelling the pie chart
for label, pct_text in zip(labels, pct_texts):
      pct_text.set_fontsize(22)
      pct_text.set_rotation(label.get_rotation())
plt.title('Loan Grade Proportions', fontsize=30)
plt.legend(labels = loan_grade['grade'], bbox_to_anchor=(1.2, 1.2), title = "Loan grade", fontsize=30)
plt.tight_layout()
plt.show()

"""Here we group the entries by the 'grade' category of loans and find the propotion of loans disbursed for each category.

This can give us an insight regarding the expected loan disbursement limit for different categories. From this we can conclude that the target audience comes three lower loan grades namely, __"A"__, __"B"__ and __"C"__

### 4. Trend in the number of credit lines. 

Here, we will look at the earliest creidt lines created by each customer. We will group by the credit lines created for each year and identify the trend in the credit line count from 1963 to 2015.
"""

credit_lines = loan_data.groupby(['earliest_credit_line']).count().reset_index()
plt.figure(figsize = (15,10))


# polynomial regression over the scatterplot
mymodel = np.poly1d(np.polyfit(credit_lines['earliest_credit_line'],credit_lines['interest_rate'], 4))
myline = np.linspace(1963, 2015, 900)

# plotting the scatter plot
plt.scatter(credit_lines['earliest_credit_line'],credit_lines['interest_rate'], color = 'green')

# labelling the plot
plt.plot(myline, mymodel(myline), color = 'red')
plt.axvline(x = 2008, color = 'b', ymin = 0, ymax = 800, label='2008')
plt.xticks(list(credit_lines['earliest_credit_line'])[::4], fontsize=18)
plt.xlabel('Years', fontsize=18)
plt.ylabel('Numer of Earliest Credit Lines', fontsize=18)
plt.title("Earliest credit lines trend", fontsize=20 )
plt.show()

"""From this plot we can observe that number of earliest credit lines created increased as the year progresseed initially. However, we see a sudden drop in the value around 2008 as per the __"blue line"__. This might have been due to the economic crisis and the impact might have continued till 2015. The shoot in the curve prior to 2008 can be considered due to the expansion of the economic bubble prior to its burst

### 5. Interest rate for riskier customers (loan borrowers) base on theit deb to income ratio

Plotting the interest rate againt normalized debt_to_income ratio
"""

# Higher the debt to income ratio riskier is the borrower. Hence higher interest rate
plt.figure(figsize = (15,8))

# normalizing the debt to income ratio
normalized_di=(loan_data['debt_to_income']-loan_data['debt_to_income'].min())/(loan_data['debt_to_income'].max()-loan_data['debt_to_income'].min())
plt.scatter(normalized_di,loan_data['interest_rate'], color = 'green')

# labelling the plot
plt.xlabel('Debt to Income Ratio', fontsize=18)
plt.ylabel('Interest Rate', fontsize=18)
plt.title("Interest Rate vs Debt to Income Ratio", fontsize=20 )
plt.xticks(fontsize=18)
plt.show()

"""Here we observe that interest rate are higher for customers with low debt_to_income ratio and vice versa. Thus we can conlude that customers with higher debt_to_income ratio can be deemed as riskier. So, debt_to_income ratio can become be an important parameter in deciding the risk factor associated with a loan applicant

## 5. Data Cleaning

After describing the data we have indentified the columns with missing values.


For cleaning the dataframe we will go column by column and analysis of each column will follow three strategies mentioned below - 


1. Missing value count being more than 60% -> Removing the column
2. Missing value count less than 60%-> Perform data imputation techniques based on distribution of columns and their correlation with other predictors

We will first start with splitting dataframe into train and test data to keep test data invisible to the model. We will apply all the tranformations and cleaning techniques of the train data to the test data
"""

from sklearn.model_selection import train_test_split
X = loan_data.drop(columns=['interest_rate'])
y = loan_data['interest_rate']
loan_data_clean, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.3)

# List of missing values for the train data
loan_data_clean.isna().sum()

"""### 1) Missing value count being more than 60%
The columns 'annual_income_joint', 'verification_income_joint', 'debt_to_income_joint' and 'months_since_90d_late' have around 70% missing values. Imputing values in these columns based on the distibution of non null points might give unexpected values and affect the prediction of outcome. Hence we will remove these columns

Similarly the column 'num_accounts_120d_past_due' either comprises of null values or 0. The value 0 means that there no current accounts that are 120 days past due. Hence we remove this column as well.

We can also drop the columns 'loan_purpose', 'emp_title', 'state' as they have too many categoeries to be considered as factors. So we will remove these columns as well.
"""

loan_data_clean = loan_data_clean.drop(columns=['annual_income_joint', 'verification_income_joint', 'debt_to_income_joint','emp_title','state','loan_purpose','num_accounts_120d_past_due','months_since_90d_late'])

X_test = X_test.drop(columns=['annual_income_joint', 'verification_income_joint', 'debt_to_income_joint','emp_title','state','loan_purpose','num_accounts_120d_past_due','months_since_90d_late'])

loan_data_clean.head()

"""### 2) Missing value being less than 60%

*emp_length* and *months_since_last_credit_inquiry*
"""

# correlation plot
corr_1 = loan_data_clean.corr()
corr_1[['emp_length','months_since_last_credit_inquiry']].T

"""The columns 'emp_length' and 'months_since_last_credit_inquiry' are not highly correlated to any other column. So we can go ahead with it imputing values with considering distributions of other columns. For imputation we will use 3 parameters - mean, median and mode

Employee experience can vary from 1 to a higher value of 60. Here all the employees with more than 10 years of experience are lables with the experience of 10 years. So, the mode is 10. Generally employees with higher experience are in the position to get settled. Since we are dealing approved loans we will go ahead with imputing the missing values with __"mode"__.

Similary for the column __"months_since_last_credit_inquiry"__ the distribution is skewed to the right.

Looking at the non normal distribution of the predictors and values of mean and median we can concluded that these values will affect the distribution of the predictor. 

Hence we will go ahead imputing the missing values with mode
"""

def mode_imputation(col_name):
  """Function to impute the missing values in a columns with mode"""
  para  = [loan_data_clean[col_name].mean(), loan_data_clean[col_name].median(),loan_data_clean[col_name].mode()[0]]
  print("Mean: ", para[0])
  print("Mean: ", para[1])
  print("Mean: ", para[2])
  # distribution plot before imputing the missing value
  sns.displot(loan_data_clean, x=col_name, kind="kde")
  plt.title("Distribution before imputation")

  loan_data_clean[col_name] = loan_data_clean[col_name].fillna(loan_data_clean[col_name].mode()[0])
  # distribution plot before imputing the missing value
  sns.displot(loan_data_clean, x=col_name, kind="kde")
  plt.title("Distribution after imputation")
  plt.show()

# mode imputation for emp_length
mode_imputation("emp_length")

# applying the same method to test data
X_test["emp_length"] = X_test['emp_length'].fillna(X_test['emp_length'].mode()[0])

# mode imputation for months_since_last_credit_inquiry
mode_imputation("months_since_last_credit_inquiry")

# applying the same method to test data
X_test["months_since_last_credit_inquiry"] = X_test['months_since_last_credit_inquiry'].fillna(X_test['months_since_last_credit_inquiry'].mode()[0])

"""*months_since_last_delinq*"""

# correlation plot
corr_months_since_last_delinq = loan_data_clean.corr()
corr_months_since_last_delinq = corr_months_since_last_delinq[['months_since_last_delinq']]
corr_months_since_last_delinq[abs(corr_months_since_last_delinq['months_since_last_delinq'])>0.5]

"""After plotting the correlation plot for the __"months_since_last_delinq"__ we observe that it is has a high negative correlation with __"delinq_2y"__, since more the Delinquencies, more is the frequnecy and less is the time between two deliquencies.

Thus, we will impute the missing values with a univariate approach. We will predict the __"months_since_last_delinq"__ column using the columns with which it is highly correlated. We will use the IterativeImputer package from sklearn

"""

# importing the pacakge
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# initializing the imputer
impute_it = IterativeImputer()

# imputing the missing values with multivariate regression
regression_imputation = loan_data_clean[['delinq_2y','months_since_last_delinq']]
a = impute_it.fit_transform(regression_imputation)
loan_data_clean['months_since_last_delinq'] =np.round(a[:,1],2)

# confirming the count of missing values as 0
print("Missing value count for train data", loan_data_clean['months_since_last_delinq'].isna().sum())

#Now we apply the same to test data
# imputing the missing values with multivariate regression
regression_imputation_test = X_test[['delinq_2y','months_since_last_delinq']]
a_test = impute_it.fit_transform(regression_imputation_test)
X_test['months_since_last_delinq'] =np.round(a_test[:,1],2)

# confirming the count of missing values as 0
print("Missing value count for test data", X_test['months_since_last_delinq'].isna().sum())

"""*debt_to_income*"""

corr_2 = loan_data_clean.corr()
corr_2[['debt_to_income']].T

"""The column __"debt_to_income"__  does not have high correlation with any other columns. 
Here the the data is right skewed so mean is out of question. Since this is a continuous variable and the mode frequency not being distinctly high with respect to other values we can go ahead with median imputation for this column
"""

# sns.displot(loan_data_clean, x="debt_to_income", kind="kde")
plt.figure(figsize=(15,7))
ax = sns.boxplot(x="debt_to_income",data=loan_data_clean, linewidth=2.5, color='yellow')
plt.xlabel('debt_to_income', fontsize=18)
print("Mean : ", loan_data_clean['debt_to_income'].mean())
print("Median : ", loan_data_clean['debt_to_income'].median())
print("Mode : ", loan_data_clean['debt_to_income'].mode())

loan_data_clean['debt_to_income'] = loan_data_clean['debt_to_income'].fillna(loan_data_clean['debt_to_income'].median())
print("The missing value count for train data", loan_data_clean['debt_to_income'].isna().sum())

#Applying same imputation to the test data
X_test['debt_to_income'] = X_test['debt_to_income'].fillna(X_test['debt_to_income'].median())
print("The missing value count for test data", X_test['debt_to_income'].isna().sum())

"""Now that we have removed all the missing values we will have to format the categorical variables

### 1. Formatting binary variables
*disbursement_method*

We can drop the binary categorical variable __"disbursement_method"__ as it is highly imbalanced. Approximately 93% of data belongs to the class __"Cash"__.
"""

loan_data_clean['disbursement_method'].value_counts()

"""
*initial_listing_status*

We can drop the binary categorical variable __"initial_listing_status"__ as it is highly imbalanced. Approximately 82% of data belongs to the class __"whole"__.
Which means there are very few instance of __"fractional"__ which can contribute towards prediction. The dataset description does not precisely mention the use of this variable. So it is better to go ahead by disregarding this variable."""

loan_data_clean['initial_listing_status'].value_counts()

"""*application_type*

The column __"application_type"__ also has a disporportionate distribution. Moreover, we have already removed certain predictors pertaining to join appliation. Hence dropping this column. We can consider joint application in a separate analysis
"""

loan_data_clean['application_type'].value_counts()

loan_data_clean = loan_data_clean.drop(columns=['initial_listing_status','disbursement_method','application_type'])

# applying the same formatting to test data
X_test = X_test.drop(columns=['initial_listing_status','disbursement_method','application_type'])


loan_data_clean.head()

"""### 2. Formatting variables with more than 2 categories
*homeownership*

The homeownership has 3 classes as mentioned below. However the risk of loan disbursement depends more on the fact that it is against a mortgage or not. Hence we can go ahead with combining the class __"OWN"__ and __"RENT"__ and __"NON_MORTGAGE"__. This also ensures that the class count is balanced.
"""

loan_data_clean['homeownership'].value_counts()

loan_data_clean.replace("OWN", "NON_MORTGAGE", inplace =True)
loan_data_clean.replace("RENT", "NON_MORTGAGE",inplace =True)

# applying the same formatting to test data
X_test.replace("OWN", "NON_MORTGAGE", inplace =True)
X_test.replace("RENT", "NON_MORTGAGE",inplace =True)


loan_data_clean['homeownership'].value_counts()

loan_data_clean['MORTGAGE'] = pd.get_dummies(loan_data_clean['homeownership'])['MORTGAGE']
loan_data_clean = loan_data_clean.drop(columns=['homeownership'])

# applying the same formatting to test data
X_test['MORTGAGE'] = pd.get_dummies(X_test['homeownership'])['MORTGAGE']
X_test = X_test.drop(columns=['homeownership'])

"""*verified_income*

This column has 3 caregories, but the distribution is not highly imbalanced. So we can make use on hot coding.

reference article - https://medium.com/analytics-vidhya/target-encoding-vs-one-hot-encoding-with-simple-examples-276a7e7b3e64
"""

from sklearn.preprocessing import OneHotEncoder 
from sklearn.preprocessing import LabelEncoder

def onehotencoding(data_1):
  """Function to perform one hot encoding on categorical data"""
  le = LabelEncoder()
  data_1['verified_income_enocded'] = le.fit_transform(data_1.verified_income)

  encoder = OneHotEncoder(categories = 'auto')
  X = encoder.fit_transform(
    data_1['verified_income_enocded'].values.reshape(-1,1)).toarray()
  dfonehot = pd.DataFrame(X, columns = ['isNotVerifed','isSourceVerified','isVerified'])

  # Concatenating the one hot encoded data to the original data frame
  data = pd.concat([data_1, dfonehot], axis =1)

  #droping encoding column
  data_1.drop(columns=['verified_income','verified_income_enocded'], inplace=True)

# Applying onehotencoding to train data
onehotencoding(loan_data_clean)

# Applying onehotencoding to test data
onehotencoding(X_test)

"""*sub_grade*

reference link - https://www.lendingclub.com/foliofn/rateDetail.action

Quoted from the reference link

"The Adjustment for Risk & Volatility is designed to cover expected losses and provide higher risk-adjusted returns for each loan grade increment from A1 to G5."

Considering this we will drop the grade columns and rank the subgrade with __"A1"__ being the lowest and __"G5"__ being the highest.

"""

def rankinggrade(data):
  """"Ranking the grade categories in a column"""
  l = sorted(list(data.sub_grade.unique()))
  rank = {}
  for i in range(len(l)):
    rank[l[i]] =i+1
  data['sub_grade_rank'] = [rank[ele] for ele in data['sub_grade']]

  data.drop(columns=['grade','sub_grade'], inplace=True)

# Applying ranking to train data
rankinggrade(loan_data_clean)

# Applying ranking to test data
rankinggrade(X_test)

"""*loan_status*

The column is highly dispropotionate with a high class imbalance within its categories. Majority of the columns are either paid or up to date on all outstanding payments.
This dataset does not have sufficient cases of the loan not being repayed or paid with a delay. Hence we can remove this column
"""

loan_data_clean = loan_data_clean.drop(columns=['loan_status'])

# Applyingt the same formatting to test data
X_test = X_test.drop(columns=['loan_status'])

"""### 3. Time series variables

*issue_month*

The issue months mentions the date of issue of the loan. We will drop this columns since the date is just 3 days of for 3 months
"""

loan_data_clean = loan_data_clean.drop(columns=['issue_month'])

# Applyingt the same formatting to test data
X_test = X_test.drop(columns=['issue_month'])

loan_data_clean.info()

"""# 6. Data Mondelling

Now we will perform regression on the data and check the model's performance on the unknown test data

We will use of 2 types of models to perform regression

## 1. Linear Regression

We will first use the linear regression to test the model
"""

from sklearn.linear_model import LinearRegression

# creating an object of LinearRegression class
X_train  = loan_data_clean
LR = LinearRegression()

# fitting the training data
LR.fit(X_train,y_train)

#prdicting the the interest values using the fitted model
y_prediction_lr =  LR.predict(X_test)
y_prediction_lr

"""For evaluating the model performance we will make use of the __"r-suared"__, __"mean squared error"__ and __"root mean squared error"__"""

# importing r2_score module
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
# predicting the accuracy score
score_lr=r2_score(y_test,y_prediction_lr)
print('r2 socre is ',score_lr)
print('mean_sqrd_error is==',mean_squared_error(y_test,y_prediction_lr))
print('root_mean_squared error of is==',np.sqrt(mean_squared_error(y_test,y_prediction_lr)))

"""## 2. Random Forest"""

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators=5, random_state=0)
regressor.fit(X_train,y_train)

#prdicting the the interest values using the fitted model
y_prediction_rf =  regressor.predict(X_test)
y_prediction_rf = np.round(y_prediction_rf,2)
y_prediction_rf

"""For evaluating the model performance we will make use of the __"r-suared"__, __"mean squared error"__ and __"root mean squared error"__"""

score_rf=r2_score(y_test,y_prediction_rf)
print('r2 socre is ',score_rf)
print('mean_sqrd_error is==',mean_squared_error(y_test,y_prediction_rf))
print('root_mean_squared error of is==',np.sqrt(mean_squared_error(y_test,y_prediction_rf)))

"""#7. Description of my approach

The approach can be divded in 6 parts

__"1. Importing libraries"__

__"2. Loading the data"__

__"3. Data description -"__

*   Identification of Missing Values
*   Indentification of Non Numerical variables
*   Displaying descriptive statistics of numverical variables

__"4. Data Visuaization - "__

Created 5 plots to visualize the trends significant factors in the loan lending lending business which can help in identifying customers with potenital risk of loan repayment

__"5. Data Cleaning - "__

My Strategy

For each variable - 

1.   Identify the missing value count
2.   If the missing count is more than 60%, remove the column
3.   If the missing count is less then 60%, identify the correlation of variable with other columns
4.   If there is no correlation then impute the missing the missing values with mean, median or mode based on its distibution
5.   If the variable as a high correlation with a magnitude above 0.5, use regression to predict the unkown variable from the know variable

This is followed by formatting the categorical variables using various encoding techniques for it to be included in the regression model

__"6. Data Modelling - "__

Using 2 regression models to predict __"interest_rate"__ based on the selected feautres.

The 2 models used where __"linear regression"__ and __"random forest regressor"__

# 8. Assumptions made

Following assumptions were made  - 



*   For the __"homeownership"__ I have assumed _"OWN"_ and 
_"RENT"_ into the same category. So the two catgories I have considered are _'MORTGAGE'_ and _'NON_MORTGAGE'_
*   Grade _'A1'_ is the lowest and grade _'G4'_ is the highest
*   All the grade cateogories are present in the train and test set

# 9. Future enhancements 

In this analysis only 2 models have been used with fixed hypre parameters as majority of the time was spent in cleaning and extracting relevant features for prediction. Given more time, more models could have been used for fitting the data. 

Neaural Networks could have been used to train the model with a back porpagation and predict the interest_rate.

The data set has around 55 predictors. Performing best subset selection which takes into account all possible predcitor combinations and can feasibly compute results for upto 40-50 predictors would have provided relevant features.
Even backward subset selection would have been a feasible option.

Lastly, the column 'emp_title' could have been used by tokenizing the words in the title and vectorizing using TF-IDF for predicting the interest_rate.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/case_study_1_analysis.ipynb

